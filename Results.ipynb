{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "for i in range(5):\n",
    "    with open(f\"xz_results_{i}.pkl\", \"rb\") as dill_file:\n",
    "        results_dict[f\"xz_{i}\"] = dill.load(dill_file)\n",
    "    with open(f\"sch_results_{i}.pkl\", \"rb\") as dill_file:\n",
    "        results_dict[f\"sch_{i}\"] = dill.load(dill_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3961503279960033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# * Obtains average tolerance threshold from XZ-method runs.\n",
    "\n",
    "averages = []\n",
    "for i in range(5):\n",
    "    result = results_dict[f\"xz_{i}\"]\n",
    "    for j in range(100):\n",
    "        averages.append(np.mean([result[j].graph.nodes[node]['object'].tolerance_threshold for node in result[j].graph.nodes if result[j].graph.nodes[node]['object'] != None and result[j].graph.nodes[node]['object'].tolerance_threshold != None]))\n",
    "np.mean(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * Empty node degree centrality analysis (do empty nodes congregate in high-degree areas?)\n",
    "\n",
    "dc_ratios = []\n",
    "for i in range(5):\n",
    "    results = results_dict[f\"xz_{i}\"]\n",
    "    dc_ratio = []\n",
    "    for k in range(100):\n",
    "        G = results[k].graph\n",
    "        dc = nx.degree_centrality(G)\n",
    "        dc_empty_nodes = []\n",
    "        dc_all_nodes = []\n",
    "        for node in G.nodes:\n",
    "            if G.nodes[node]['object'] == None:\n",
    "                dc_empty_nodes.append(dc[node]*len(G))\n",
    "            dc_all_nodes.append(dc[node]*len(G))\n",
    "        dc_ratios.append([\"Xie & Zhou, Vacancies\",i*32,np.mean(dc_empty_nodes)])\n",
    "        dc_ratios.append([\"Xie & Zhou, All Nodes\",i*32,np.mean(dc_all_nodes)])\n",
    "    #dc_ratios[f\"Xie & Zhou, {i*32}\"]= dc_ratio\n",
    "    \n",
    "    results = results_dict[f\"sch_{i}\"]\n",
    "    dc_ratio = []\n",
    "    for k in range(100):\n",
    "        G = results[k].graph\n",
    "        dc = nx.degree_centrality(G)\n",
    "        dc_empty_nodes = []\n",
    "        dc_all_nodes = []\n",
    "        for node in G.nodes:\n",
    "            if G.nodes[node]['object'] == None:\n",
    "                dc_empty_nodes.append(dc[node]*len(G))\n",
    "            dc_all_nodes.append(dc[node]*len(G))\n",
    "        dc_ratios.append([\"Schelling, Vacancies\",i*32,np.mean(dc_empty_nodes)])\n",
    "        dc_ratios.append([\"Schelling, All Nodes\",i*32,np.mean(dc_all_nodes)])\n",
    "    #dc_ratios[f\"Schelling, {i*32}\"] = dc_ratio\n",
    "\n",
    "df = pd.DataFrame(dc_ratios)\n",
    "df = df.rename(columns={0:\"Method, Group\",1:\"Densifications\",2:\"Average Degree\"})\n",
    "df\n",
    "sns.stripplot(data=df, size=2, x='Densifications', y='Average Degree', hue=\"Method, Group\",palette='colorblind', dodge=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * Node degree centrality analysis (do tolerant nodes congregate in high-degree areas?)\n",
    "\n",
    "dc_ratios = []\n",
    "for i in range(5):\n",
    "    results = results_dict[f\"xz_{i}\"]\n",
    "    dc_tolerant = []\n",
    "    dc_intolerant = []\n",
    "    for k in range(100):\n",
    "        G = results[k].graph\n",
    "        dc = nx.degree_centrality(G)\n",
    "        dc_tolerant_nodes = []\n",
    "        dc_intolerant_nodes = []\n",
    "        dc_all_nodes = []\n",
    "        for node in G.nodes:\n",
    "            if G.nodes[node]['object'] != None and G.nodes[node]['object'].tolerance_threshold != None:\n",
    "                if G.nodes[node]['object'].tolerance_threshold >= 0.57:\n",
    "                    dc_tolerant_nodes.append(dc[node]*len(G))\n",
    "                elif G.nodes[node]['object'].tolerance_threshold <= 0.07:\n",
    "                    dc_intolerant_nodes.append(dc[node]*len(G))\n",
    "                dc_all_nodes.append(dc[node]*len(G))\n",
    "        dc_ratios.append([\"Tolerant\", i*32, np.mean(dc_tolerant_nodes)])\n",
    "        dc_ratios.append([\"Intolerant\", i*32, np.mean(dc_intolerant_nodes)])\n",
    "        dc_ratios.append([\"All Nodes\", i*32, np.mean(dc_all_nodes)])\n",
    "    #dc_ratios[f\"Tolerant, {i*32}\"]= dc_tolerant\n",
    "    #dc_ratios[f\"Intolerant, {i*32}\"]= dc_intolerant\n",
    "\n",
    "df = pd.DataFrame(dc_ratios)\n",
    "df = df.rename(columns={0:'Node Type',1:\"Densifications\",2:\"Average Degree\"})\n",
    "#df\n",
    "sns.stripplot(data=df, size=2, x='Densifications', y='Average Degree', hue='Node Type',palette='colorblind', dodge=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * Plots change in assortativity (Xie & Zhou)\n",
    "\n",
    "\n",
    "average_results = []\n",
    "\n",
    "for i in range(5):\n",
    "    results = results_dict[f\"sch_{i}\"]\n",
    "    array = np.array([(r.attribute_assortativity) for r in results])\n",
    "    average_array = np.average(array,axis=0)\n",
    "    average_results.append([\"Schelling\", i*32, average_array])\n",
    "    results = results_dict[f\"xz_{i}\"]\n",
    "    array = np.array([(r.attribute_assortativity) for r in results])\n",
    "    average_array = np.average(array,axis=0)\n",
    "    average_results.append([\"Xie & Zhou\", i*32, average_array])\n",
    "\n",
    "df = pd.DataFrame(average_results)\n",
    "df = df.rename(columns={0:'Tolerances',1:\"Densifications\",2:\"Assortativity\"})\n",
    "df = df.explode(\"Assortativity\")\n",
    "df['t'] = df.groupby(['Tolerances','Densifications']).cumcount()+1\n",
    "sns.lineplot(data=df, x='t', y='Assortativity', hue='Densifications', palette='colorblind',style='Tolerances')\n",
    "plt.xscale('linear')\n",
    "plt.yscale('logit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "assort_results = []\n",
    "for i in range(5):\n",
    "    results = results_dict[f\"sch_{i}\"]\n",
    "    array = np.array([(r.attribute_assortativity) for r in results])\n",
    "    average_array = np.average(array,axis=0)\n",
    "    speed_1 = (average_array[250]-average_array[0])\n",
    "    speed_2 = (average_array[4000]-average_array[3750])\n",
    "    level_1 = average_array[250]\n",
    "    level_2 = average_array[2500]\n",
    "    assort_results.append(['Schelling', i*32,speed_1,level_1,speed_2,level_2])\n",
    "    results = results_dict[f\"xz_{i}\"]\n",
    "    array = np.array([(r.attribute_assortativity) for r in results])\n",
    "    average_array = np.average(array,axis=0)\n",
    "    speed_1 = (average_array[250]-average_array[0])\n",
    "    speed_2 = (average_array[4000]-average_array[3750])\n",
    "    level_1 = average_array[250]\n",
    "    level_2 = average_array[2500]\n",
    "    assort_results.append(['Xie & Zhou', i*32,speed_1,level_1,speed_2,level_2])\n",
    "\n",
    "df = pd.DataFrame(assort_results)\n",
    "df = df.rename(columns={0:'Tolerances',1:\"Densifications\",2:fr'$\\Delta$ Assortativity (first 250 steps)',3:\"Assortativity at t=250\",4:fr'$\\Delta$ Assortativity (last 250 steps)',5:\"Assortativity at t=4000\"})\n",
    "#df = df.explode(\"Assortativity\")\n",
    "#df['t'] = df.groupby(['Tolerances','Densifications']).cumcount()+1\n",
    "#sns.scatterplot(data=df, x=fr'$\\Delta$ Assortativity (first 250 steps)', y='Assortativity at t=250', hue='Densifications', palette='colorblind',style='Tolerances')\n",
    "#plt.xscale('linear')\n",
    "#plt.yscale('logit')\n",
    "#plt.show()\n",
    "#sns.scatterplot(data=df, x=fr'$\\Delta$ Assortativity (last 250 steps)', y='Assortativity at t=4000', hue='Densifications', palette='colorblind',style='Tolerances')\n",
    "#plt.show()\n",
    "sns.scatterplot(data=df, x=fr'$\\Delta$ Assortativity (first 250 steps)', y='Assortativity at t=4000', hue='Densifications', palette='colorblind',style='Tolerances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4001"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_results = []\n",
    "\n",
    "for i in range(5):\n",
    "    results = results_dict[f\"xz_{i}\"]\n",
    "    array = np.array([(r.attribute_assortativity) for r in results])\n",
    "    average_array = np.average(array,axis=0)\n",
    "    average_results.append([\"Xie & Zhou\", i*32, average_array])\n",
    "\n",
    "df = pd.DataFrame(average_results)\n",
    "df = df.rename(columns={0:'Tolerances',1:\"Densifications\",2:\"Assortativity\"})\n",
    "df = df.explode(\"Assortativity\")\n",
    "df['t'] = df.groupby(['Tolerances','Densifications']).cumcount()+1\n",
    "sns.lineplot(data=df, x='t', y='Assortativity', hue='Densifications', palette='colorblind',style='Densifications')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * Plots n Final Connected Components (Xie & Zhou)\n",
    "\n",
    "fig, ax  = plt.subplots()\n",
    "\n",
    "plt_array = []\n",
    "for i in range(5):\n",
    "    results = results_dict[f\"xz_{i}\"]\n",
    "    plt_array.append(['Xie & Zhou', i*32, list(np.array([(r.n_connected_components[-1]) for r in results]))])\n",
    "    results = results_dict[f\"sch_{i}\"]\n",
    "    plt_array.append(['Schelling', i*32, list(np.array([(r.n_connected_components[-1]) for r in results]))])\n",
    "\n",
    "df = pd.DataFrame(plt_array)\n",
    "df = df.rename(columns={0:'Preferences',1:\"Densifications\",2:\"Final Connected Components\"})\n",
    "df = df.explode(\"Final Connected Components\",ignore_index=True)\n",
    "#df['t'] = df.groupby(['Preferences','Densifications']).cumcount()+1\n",
    "temp_dict = df.to_dict()\n",
    "df = pd.DataFrame(temp_dict)\n",
    "sns.violinplot(data=df, x='Densifications', y='Final Connected Components', hue='Preferences',palette='colorblind', dodge=True, alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt_array\u001b[39m.\u001b[39;49mshape()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "plt_array.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * Plots Final CC Size\n",
    "\n",
    "\n",
    "plt_array = []\n",
    "for i in range(5):\n",
    "    results = results_dict[f\"xz_{i}\"]\n",
    "    plt_array.append(['Xie & Zhou', i*32, list(np.array([(r.std_cc_size[-1]) for r in results]))])\n",
    "    results = results_dict[f\"sch_{i}\"]\n",
    "    plt_array.append(['Schelling', i*32, list(np.array([(r.std_cc_size[-1]) for r in results]))])\n",
    "\n",
    "df = pd.DataFrame(plt_array)\n",
    "df = df.rename(columns={0:'Preferences',1:\"Densifications\",2:\"Final Average Connected Component Size\"})\n",
    "df = df.explode(\"Final Average Connected Component Size\",ignore_index=True)\n",
    "#df['t'] = df.groupby(['Preferences','Densifications']).cumcount()+1\n",
    "temp_dict = df.to_dict()\n",
    "df = pd.DataFrame(temp_dict)\n",
    "sns.violinplot(data=df, x='Densifications', y='Final Average Connected Component Size', hue='Preferences',palette='colorblind', dodge=True,size=10, alpha=.05,)\n",
    "#sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deter\\AppData\\Local\\Temp\\ipykernel_38492\\2264868808.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  array.append([i*32, list(np.array([(nx.degree_histogram(r.graph)) for r in results]))])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# * Plots degree distributions by n densifications\n",
    "\n",
    "array = []\n",
    "\n",
    "for i in range(5):\n",
    "    results = results_dict[f\"xz_{i}\"]\n",
    "    array.append([i*32, list(np.array([(nx.degree_histogram(r.graph)) for r in results]))])\n",
    "\n",
    "df = pd.DataFrame(array)\n",
    "df = df.rename(columns={0:'Densifications',1:\"Degree Histograms\"})\n",
    "df = df.explode(\"Degree Histograms\",ignore_index=True)\n",
    "df = df.explode(\"Degree Histograms\")\n",
    "df = df.rename(columns={\"Degree Histograms\":\"Frequency\"})\n",
    "df['Degree'] = df.groupby(df.index).cumcount()+1\n",
    "sns.lineplot(data=df, x='Degree', y='Frequency', hue='Densifications', palette='colorblind',style='Densifications',markers=True)\n",
    "plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Densifications</th>\n",
       "      <th>Degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1024.0</td>\n",
       "      <td>1024.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>96.0</td>\n",
       "      <td>12.476562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.578810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>96.0</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>96.0</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>96.0</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>96.0</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>96.0</td>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Densifications       Degree\n",
       "count          1024.0  1024.000000\n",
       "mean             96.0    12.476562\n",
       "std               0.0     7.578810\n",
       "min              96.0     2.000000\n",
       "25%              96.0     4.000000\n",
       "50%              96.0    12.000000\n",
       "75%              96.0    17.000000\n",
       "max              96.0    37.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# * Plots mean degree observed by n densifications.\n",
    "\n",
    "array = []\n",
    "\n",
    "for i in range(5):\n",
    "    results = results_dict[f\"xz_{i}\"]\n",
    "    degree_list = [list(r.graph.degree) for r in results]\n",
    "    degree_list = degree_list[0]\n",
    "    degree_list = [d[1] for d in degree_list]\n",
    "    array.append([i*32, degree_list])\n",
    "\n",
    "df = pd.DataFrame(array)\n",
    "df = df.rename(columns={0:'Densifications',1:\"Degree\"})\n",
    "df = df.explode(\"Degree\")\n",
    "df['Degree'] = df['Degree'].astype(int)\n",
    "#df['Degree'] = df.groupby(df.index).cumcount()+1\n",
    "sns.violinplot(data=df, x='Densifications', y='Degree', hue='Densifications', palette='colorblind', cut=0, dodge=False)\n",
    "df[df['Densifications']==96].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * Draws selected network displays individual color.\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "ax = plt.gca()\n",
    "#ax.set_title(\"Xie & Zhou, 0 Densifications\")\n",
    "G = results_dict[\"sch_0\"][2].graph\n",
    "color_map = []\n",
    "pos = dict((n,n) for n in G.nodes)\n",
    "for node in G.nodes:\n",
    "    if G.nodes[node]['object'] == None:\n",
    "        color_map.append('black')\n",
    "    elif G.nodes[node]['object'].color == 0:\n",
    "        color_map.append('red')\n",
    "    else:\n",
    "        color_map.append('blue')\n",
    "nx.draw_networkx_nodes(G, pos = pos, node_size=30, node_color= color_map, ax=ax,edgecolors='grey')\n",
    "nx.draw_networkx_edges(G, pos=pos, alpha = .3, width = 3, edge_color='grey')\n",
    "plt.scatter([],[], color='black', label='Vacancy')\n",
    "ax.legend(loc='lower center', bbox_to_anchor=(0.5, -.02),\n",
    "          ncol=3, fancybox=False, shadow=False)\n",
    "plt.tight_layout()\n",
    "plt.box(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Draws selected network displays tolerance.\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "ax = plt.gca()\n",
    "#ax.set_title(\"Xie & Zhou, 0 Densifications\")\n",
    "G = results_dict[\"xz_4\"][0].graph\n",
    "color_map = []\n",
    "pos = dict((n,n) for n in G.nodes)\n",
    "for node in G.nodes:\n",
    "    if G.nodes[node]['object'] == None:\n",
    "        color_map.append('black')\n",
    "    elif G.nodes[node]['object'].tolerance_threshold != None:\n",
    "        tol = G.nodes[node]['object'].tolerance_threshold\n",
    "        if tol <= .07:\n",
    "            color_map.append(plt.cm.RdYlGn(0))\n",
    "        elif tol <= .21:\n",
    "            color_map.append(plt.cm.RdYlGn(.25))\n",
    "        elif tol <= .36:\n",
    "            color_map.append(plt.cm.RdYlGn(.5))\n",
    "        elif tol <= .57:\n",
    "            color_map.append(plt.cm.RdYlGn(.75))\n",
    "        else:\n",
    "            color_map.append(plt.cm.RdYlGn(.99))\n",
    "    else:\n",
    "        color_map.append('violet')\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos = pos, node_size=30, node_color= color_map, ax=ax,edgecolors='grey')\n",
    "nx.draw_networkx_edges(G, pos=pos, alpha = .3, width = 3, edge_color='grey')\n",
    "for v in [[0,0.07],[.25,0.21],[.5,0.36],[.75,0.57],[.99,1.00]]:\n",
    "    plt.scatter([],[], c=[plt.cm.RdYlGn(v[0]/1)],label=fr'$\\epsilon={v[1]}$')\n",
    "plt.scatter([],[], color='violet', label='Non-Guttman')\n",
    "plt.scatter([],[], color='black', label='Vacancy')\n",
    "ax.legend(loc='lower center', bbox_to_anchor=(0.5, -.12),\n",
    "          ncol=3, fancybox=False, shadow=False)\n",
    "plt.tight_layout()\n",
    "plt.box(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * Results table\n",
    "\n",
    "col_heads_xz = ['Densifications', 'Mean Assortativity', 'Std Assortativity', 'Mean Connected Components', 'Std Connected Components', 'Mean Vacancy Degree Centrality', 'Std Vacancy Degree Centrality','Mean T Node Degree Centrality', 'Std T Node Degree Centrality','Mean I Node Degree Centrality','Std I Node Degree Centrality']\n",
    "col_heads_sch = ['Densifications', 'Mean Assortativity', 'Std Assortativity', 'Mean Connected Components', 'Std Connected Components', 'Mean Vacancy Degree Centrality', 'Std Vacancy Degree Centrality']\n",
    "xz_df = pd.DataFrame(columns=col_heads_xz)\n",
    "sch_df = pd.DataFrame(columns=col_heads_sch)\n",
    "for i in range(5):\n",
    "    results = results_dict[f\"xz_{i}\"]\n",
    "    densifications = int(i * 32)\n",
    "    mean_assortativity = np.mean([(r.attribute_assortativity[-1]) for r in results])\n",
    "    std_assortativity = np.std([(r.attribute_assortativity[-1]) for r in results])\n",
    "    mean_cc = np.mean([(r.n_connected_components[-1]) for r in results])\n",
    "    std_cc = np.std([(r.n_connected_components[-1]) for r in results])\n",
    "    mean_graph_vacancy_dc = []\n",
    "    for k in range(100):\n",
    "        G = results[k].graph\n",
    "        dc = nx.degree_centrality(G)\n",
    "        dc_empty_nodes = []\n",
    "        for node in G.nodes:\n",
    "            if G.nodes[node]['object'] == None:\n",
    "                dc_empty_nodes.append(dc[node])\n",
    "        mean_graph_vacancy_dc.append(np.mean(dc_empty_nodes))\n",
    "    mean_population_vacancy_dc = np.mean(mean_graph_vacancy_dc)\n",
    "    std_population_vacancy_dc = np.std(mean_graph_vacancy_dc)\n",
    "\n",
    "    mean_graph_T_dc = []\n",
    "    mean_graph_I_dc = []\n",
    "    for k in range(100):\n",
    "        G = results[k].graph\n",
    "        dc = nx.degree_centrality(G)\n",
    "        dc_tolerant_nodes = []\n",
    "        dc_intolerant_nodes = []\n",
    "        for node in G.nodes:\n",
    "            if G.nodes[node]['object'] != None and G.nodes[node]['object'].tolerance_threshold != None:\n",
    "                if G.nodes[node]['object'].tolerance_threshold >= 0.57:\n",
    "                    dc_tolerant_nodes.append(dc[node])\n",
    "                elif G.nodes[node]['object'].tolerance_threshold <= 0.07:\n",
    "                    dc_intolerant_nodes.append(dc[node])\n",
    "        mean_graph_T_dc.append(np.mean(dc_tolerant_nodes))\n",
    "        mean_graph_I_dc.append(np.mean(dc_intolerant_nodes))\n",
    "    mean_population_T_dc = np.mean(mean_graph_T_dc)\n",
    "    std_population_T_dc = np.std(mean_graph_T_dc)\n",
    "    mean_population_I_dc = np.mean(mean_graph_I_dc)\n",
    "    std_population_I_dc = np.std(mean_graph_I_dc)\n",
    "\n",
    "    xz_df.loc[len(xz_df.index)] = [densifications, mean_assortativity, std_assortativity, mean_cc, std_cc, mean_population_vacancy_dc, std_population_vacancy_dc, mean_population_T_dc, std_population_T_dc, mean_population_I_dc, std_population_I_dc]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    results = results_dict[f\"sch_{i}\"]\n",
    "    densifications = int(i * 32)\n",
    "    mean_assortativity = np.mean([(r.attribute_assortativity[-1]) for r in results])\n",
    "    std_assortativity = np.std([(r.attribute_assortativity[-1]) for r in results])\n",
    "    mean_cc = np.mean([(r.n_connected_components[-1]) for r in results])\n",
    "    std_cc = np.std([(r.n_connected_components[-1]) for r in results])\n",
    "    mean_graph_vacancy_dc = []\n",
    "    for k in range(100):\n",
    "        G = results[k].graph\n",
    "        dc = nx.degree_centrality(G)\n",
    "        dc_empty_nodes = []\n",
    "        for node in G.nodes:\n",
    "            if G.nodes[node]['object'] == None:\n",
    "                dc_empty_nodes.append(dc[node])\n",
    "        mean_graph_vacancy_dc.append(np.mean(dc_empty_nodes))\n",
    "    mean_population_vacancy_dc = np.mean(mean_graph_vacancy_dc)\n",
    "    std_population_vacancy_dc = np.std(mean_graph_vacancy_dc)\n",
    "\n",
    "\n",
    "    sch_df.loc[len(sch_df.index)] = [densifications, mean_assortativity, std_assortativity, mean_cc, std_cc, mean_population_vacancy_dc, std_population_vacancy_dc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col_heads_xz = ['Densifications', 'Mean Assortativity', 'Std Assortativity', 'Mean Connected Components', 'Std Connected Components', 'Mean Vacancy Degree Centrality', 'Std Vacancy Degree Centrality',#'Mean T Node Degree Centrality', 'Std T Node Degree Centrality','Mean I Node Degree Centrality','Std I Node Degree Centrality']\n",
    "#col_heads_sch = ['Densifications', 'Mean Assortativity', 'Std Assortativity', 'Mean Connected Components', 'Std Connected Components', 'Mean Vacancy Degree Centrality', 'Std Vacancy Degree Centrality']\n",
    "#xz_df = pd.DataFrame(columns=col_heads_xz)\n",
    "#sch_df = pd.DataFrame(columns=col_heads_sch)\n",
    "result_list = []\n",
    "for i in range(0,5):\n",
    "    results = results_dict[f\"xz_{i}\"]\n",
    "    densifications = int(i * 32)\n",
    "    name = 'Xie & Zhou'\n",
    "    for r in results:\n",
    "        G = r.graph\n",
    "        dc = nx.degree_centrality(G)\n",
    "        dc_empty_nodes = []\n",
    "        for node in G.nodes:\n",
    "            if G.nodes[node]['object'] == None:\n",
    "                dc_empty_nodes.append(dc[node])\n",
    "        mean_vacancy_dc = np.mean(dc_empty_nodes)\n",
    "        dc = nx.degree_centrality(G)\n",
    "        dc_tolerant_nodes = []\n",
    "        dc_intolerant_nodes = []\n",
    "        for node in G.nodes:\n",
    "            if G.nodes[node]['object'] != None and G.nodes[node]['object'].tolerance_threshold != None:\n",
    "                if G.nodes[node]['object'].tolerance_threshold >= 0.57:\n",
    "                    dc_tolerant_nodes.append(dc[node])\n",
    "                elif G.nodes[node]['object'].tolerance_threshold <= 0.07:\n",
    "                    dc_intolerant_nodes.append(dc[node])\n",
    "        mean_T_dc =np.mean(dc_tolerant_nodes)\n",
    "        mean_I_dc = np.mean(dc_intolerant_nodes)\n",
    "\n",
    "\n",
    "        result_list.append([name, densifications, r.attribute_assortativity[-1],r.n_connected_components[-1], mean_vacancy_dc, mean_T_dc, mean_I_dc])\n",
    "\n",
    "    results = results_dict[f\"sch_{i}\"]\n",
    "    densifications = int(i * 32)\n",
    "    name = 'Schelling'\n",
    "    mean_graph_vacancy_dc = []\n",
    "    for r in results:\n",
    "        G = r.graph\n",
    "        dc = nx.degree_centrality(G)\n",
    "        dc_empty_nodes = []\n",
    "        for node in G.nodes:\n",
    "            if G.nodes[node]['object'] == None:\n",
    "                dc_empty_nodes.append(dc[node])\n",
    "        mean_vacacny_dc = np.mean(dc_empty_nodes)\n",
    "        result_list.append([name, densifications, r.attribute_assortativity[-1],r.n_connected_components[-1], mean_vacacny_dc, None, None])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result_list,columns=['Tolerances','Densifications','Assortativity','Connected Components','Vacancy Degree Centrality', 'Tolerant Node Degree Centrality', 'Intolerant Node Degree Centrality'])\n",
    "\n",
    "df2 = df.melt(['Tolerances','Densifications','Assortativity','Connected Components','Vacancy Degree Centrality'],var_name='Type',value_name='Degree Centrality').copy()\n",
    "df2['Type'] = df2['Type'].apply(lambda x: 'Intolerant' if x == 'Intolerant Node Degree Centrality' else 'Tolerant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=df2, x='Densifications', y='Degree Centrality', hue='Type', palette='colorblind', cut=0, dodge=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tolerances</th>\n",
       "      <th>Densifications</th>\n",
       "      <th>Assortativity</th>\n",
       "      <th>Connected Components</th>\n",
       "      <th>Vacancy Degree Centrality</th>\n",
       "      <th>Tolerant Node Degree Centrality</th>\n",
       "      <th>Intolerant Node Degree Centrality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xie &amp; Zhou</td>\n",
       "      <td>0</td>\n",
       "      <td>0.846793</td>\n",
       "      <td>3</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.003820</td>\n",
       "      <td>0.003740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xie &amp; Zhou</td>\n",
       "      <td>0</td>\n",
       "      <td>0.850486</td>\n",
       "      <td>3</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>0.003778</td>\n",
       "      <td>0.003733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xie &amp; Zhou</td>\n",
       "      <td>0</td>\n",
       "      <td>0.853509</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.003798</td>\n",
       "      <td>0.003696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xie &amp; Zhou</td>\n",
       "      <td>0</td>\n",
       "      <td>0.859837</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003824</td>\n",
       "      <td>0.003748</td>\n",
       "      <td>0.003767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xie &amp; Zhou</td>\n",
       "      <td>0</td>\n",
       "      <td>0.818101</td>\n",
       "      <td>3</td>\n",
       "      <td>0.003850</td>\n",
       "      <td>0.003791</td>\n",
       "      <td>0.003753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Schelling</td>\n",
       "      <td>128</td>\n",
       "      <td>0.929851</td>\n",
       "      <td>3</td>\n",
       "      <td>0.014686</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Schelling</td>\n",
       "      <td>128</td>\n",
       "      <td>0.952383</td>\n",
       "      <td>2</td>\n",
       "      <td>0.013849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Schelling</td>\n",
       "      <td>128</td>\n",
       "      <td>0.960981</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Schelling</td>\n",
       "      <td>128</td>\n",
       "      <td>0.960717</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Schelling</td>\n",
       "      <td>128</td>\n",
       "      <td>0.913917</td>\n",
       "      <td>3</td>\n",
       "      <td>0.013259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tolerances  Densifications  Assortativity  Connected Components  \\\n",
       "0    Xie & Zhou               0       0.846793                     3   \n",
       "1    Xie & Zhou               0       0.850486                     3   \n",
       "2    Xie & Zhou               0       0.853509                     4   \n",
       "3    Xie & Zhou               0       0.859837                     1   \n",
       "4    Xie & Zhou               0       0.818101                     3   \n",
       "..          ...             ...            ...                   ...   \n",
       "995   Schelling             128       0.929851                     3   \n",
       "996   Schelling             128       0.952383                     2   \n",
       "997   Schelling             128       0.960981                     1   \n",
       "998   Schelling             128       0.960717                     1   \n",
       "999   Schelling             128       0.913917                     3   \n",
       "\n",
       "     Vacancy Degree Centrality  Tolerant Node Degree Centrality  \\\n",
       "0                     0.003802                         0.003820   \n",
       "1                     0.003863                         0.003778   \n",
       "2                     0.003840                         0.003798   \n",
       "3                     0.003824                         0.003748   \n",
       "4                     0.003850                         0.003791   \n",
       "..                         ...                              ...   \n",
       "995                   0.014686                              NaN   \n",
       "996                   0.013849                              NaN   \n",
       "997                   0.013525                              NaN   \n",
       "998                   0.013289                              NaN   \n",
       "999                   0.013259                              NaN   \n",
       "\n",
       "     Intolerant Node Degree Centrality  \n",
       "0                             0.003740  \n",
       "1                             0.003733  \n",
       "2                             0.003696  \n",
       "3                             0.003767  \n",
       "4                             0.003753  \n",
       "..                                 ...  \n",
       "995                                NaN  \n",
       "996                                NaN  \n",
       "997                                NaN  \n",
       "998                                NaN  \n",
       "999                                NaN  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4.0 0.0\n",
      "10.0 3.16902458764651\n",
      "40.187999999999995 6.36134574494645\n",
      "1\n",
      "3.9821 1.9818984736661736\n",
      "10.0 3.149741638171246\n",
      "34.8699 20.280274475927193\n",
      "2\n",
      "3.98 4.661476520581191\n",
      "10.0 3.1537977291431867\n",
      "41.277699999999996 38.32002397637522\n",
      "3\n",
      "4.0 0.6129003727988132\n",
      "10.0 3.162650515021086\n",
      "34.91029999999999 9.14228345626295\n"
     ]
    }
   ],
   "source": [
    "graphs = [[nx.grid_2d_graph(20,20, periodic=True) for _ in range(100)], [nx.erdos_renyi_graph(400, .01) for _ in range(100)], [nx.barabasi_albert_graph(400, 2) for _ in range(100)], [nx.watts_strogatz_graph(400,4,.1) for _ in range(100)]]\n",
    "for k in range(len(graphs)):\n",
    "    print(k)\n",
    "    means = []\n",
    "    stds = []\n",
    "    for l in range(len(graphs[k])):\n",
    "        hist = nx.degree_histogram(graphs[k][l])\n",
    "        dist = []\n",
    "        for i in range(len(hist)):\n",
    "            for j in range(hist[i]):\n",
    "                dist.append(i)\n",
    "        means.append(np.mean(dist))\n",
    "        stds.append(np.std(dist))\n",
    "    m_neighbors, std_neighbors = np.mean(means), np.mean(stds)\n",
    "    print(m_neighbors, std_neighbors)\n",
    "\n",
    "    means = []\n",
    "    stds = []\n",
    "    for _ in range(100):\n",
    "        buckets = np.zeros(400)\n",
    "        for i in range(4000):\n",
    "            buckets[np.random.randint(0,400)] += 1\n",
    "        means.append(np.mean(buckets))\n",
    "        stds.append(np.std(buckets))\n",
    "    print(np.mean(means), np.mean(stds))\n",
    "\n",
    "    means = []\n",
    "    stds = []\n",
    "    for _ in range(100):\n",
    "        groups = []\n",
    "        for _ in range(100):\n",
    "            groups.append(sum(np.random.choice(buckets,size=(max(int(np.random.normal(m_neighbors/10,std_neighbors/10)*10),0)),replace=False)))\n",
    "        means.append(np.mean(groups))\n",
    "        stds.append(np.std(groups))\n",
    "    print(np.mean(means),np.mean(stds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_entropy(prob_arr):\n",
    "    log_arr = np.array([np.log2(x) for x in prob_arr])\n",
    "    shannon = -1* sum([x*y for x,y in list(zip(prob_arr,log_arr))])\n",
    "    print(f\"Shannon: {shannon}\")\n",
    "\n",
    "def discretize(x,bins=50):\n",
    "    div = bins/100\n",
    "    buckets = np.arange(0,1,1/bins)\n",
    "    if x == -1:\n",
    "        return -1\n",
    "    elif x == -2:\n",
    "        return -2\n",
    "    else:\n",
    "        for i in range(len(buckets)):\n",
    "            if x > buckets[i]:\n",
    "                discrete_x = int((i/div))\n",
    "        return discrete_x\n",
    "\n",
    "def process_array(array,xlabel,ylabel):\n",
    "    for d in array:\n",
    "        df = pd.DataFrame(d)\n",
    "        df = df.sort_values(by=0,ascending=False)\n",
    "        df['Pair'] = \"(\" + df[0].astype(str) + \", \" + df[1].astype(str) + \")\"\n",
    "        probabilities = []\n",
    "        sorted_symbols = pd.DataFrame([str(sorted((x[1][0],x[1][1]))) for x in df.iterrows()])\n",
    "        for symbol in set(sorted_symbols[0]):\n",
    "            relative_freq = sorted_symbols[0].value_counts()[symbol]/len(sorted_symbols[0])\n",
    "            probabilities.append(relative_freq)\n",
    "        measure_entropy(probabilities)\n",
    "        df = df.groupby([0,1]).count().reset_index()\n",
    "        df['Pair'] = df['Pair'].apply(lambda x: np.log(x))\n",
    "        df = df.pivot_table(index=0,columns=1,values='Pair')\n",
    "        #df = df.fillna(0)\n",
    "        sns.heatmap(df,cmap='rocket_r',square=True).invert_yaxis()\n",
    "        plt.xlabel(f\"{ylabel}\")\n",
    "        plt.ylabel(f\"{xlabel}\")\n",
    "        plt.show()\n",
    "        #sns.heatmap(df.corr(),cmap='Spectral_r',square=True).invert_yaxis()\n",
    "        #plt.xlabel(f\"{ylabel}\")\n",
    "        #plt.ylabel(f\"{ylabel}\")\n",
    "        #lt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = [[nx.grid_2d_graph(20,20, periodic=True) for _ in range(100)], [nx.erdos_renyi_graph(400, .01) for _ in range(100)], [nx.barabasi_albert_graph(400, 2) for _ in range(100)], [nx.watts_strogatz_graph(400,4,.1) for _ in range(100)]]\n",
    "degree_array = [[],[],[],[],[]]\n",
    "pair_array = [[],[],[],[],[]]\n",
    "for k in range(len(graphs)):\n",
    "    for l in range(len(graphs[k])):\n",
    "        G = graphs[k][l]\n",
    "\n",
    "        for node in G.nodes:\n",
    "            degree = G.degree(node)\n",
    "            degree_array[k].append(degree)\n",
    "            for nbr in G.neighbors(node):\n",
    "                nbr_degree = G.degree(nbr)\n",
    "                pair_array[k].append(str(sorted((degree,nbr_degree))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon: -0.0\n",
      "Shannon: 2.999105639298715\n",
      "Shannon: 2.516138464352316\n",
      "Shannon: 1.3214721651588783\n",
      "Shannon: 0\n",
      "Shannon: -0.0\n",
      "Shannon: 5.148017541431519\n",
      "Shannon: 7.226564862273881\n",
      "Shannon: 2.160085238477084\n",
      "Shannon: 0\n"
     ]
    }
   ],
   "source": [
    "degree_freq_array = [[],[],[],[],[]]\n",
    "pair_freq_array = [[],[],[],[],[]]\n",
    "for i in range(len(degree_array)):\n",
    "    freq_array = []\n",
    "    for elem in set(degree_array[i]):\n",
    "        proportion = degree_array[i].count(elem) / len(degree_array[i])\n",
    "        freq_array.append(proportion)\n",
    "    degree_freq_array[i] = freq_array\n",
    "\n",
    "for i in range(len(pair_array)):\n",
    "    freq_array = []\n",
    "    for elem in set(pair_array[i]):\n",
    "        proportion = pair_array[i].count(elem) / len(pair_array[i])\n",
    "        freq_array.append(proportion)\n",
    "    pair_freq_array[i] = freq_array\n",
    "\n",
    "for arr in degree_freq_array:\n",
    "    measure_entropy(arr)\n",
    "\n",
    "for arr in pair_freq_array:\n",
    "    measure_entropy(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon: 4.847058650911167\n",
      "Shannon: 8.969872932273288\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### * Plots tolerance histogram \n",
    "\n",
    "array = []\n",
    "\n",
    "for i in range(1):\n",
    "    results = results_dict[f\"xz_{i}\"]\n",
    "    G = results[0].graph\n",
    "    for node in G:\n",
    "        if G.nodes[node]['object'] != None:\n",
    "            if G.nodes[node]['object'].tolerance_threshold != None:\n",
    "                array.append(G.nodes[node]['object'].tolerance_threshold)\n",
    "df = pd.DataFrame(array)\n",
    "df = df.rename(columns={0:'Tolerance Threshold'})\n",
    "sns.histplot(data=df, bins=25)\n",
    "#plt.xlabel('Tolerance Threshold')\n",
    "plt.show()\n",
    "\n",
    "tlist = df['Tolerance Threshold'].apply(discretize).to_list()\n",
    "pair_list = [tuple(sorted((np.random.choice(tlist),np.random.choice(tlist)))) for _ in range(7567)]\n",
    "for i in range(int(32*32*.15)):\n",
    "    tlist.append(-1)\n",
    "prob_array = []\n",
    "for val in set(tlist):\n",
    "    prob_array.append(tlist.count(val)/len(tlist))\n",
    "pair_prob_array = []\n",
    "for val in set(pair_list):\n",
    "    pair_prob_array.append(pair_list.count(val)/len(pair_list))\n",
    "\n",
    "measure_entropy(prob_array)\n",
    "measure_entropy(pair_prob_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon: 0.6824073815619667\n",
      "Shannon: 4.877636216614147\n",
      "Shannon: 6.312244461516681\n",
      "Shannon: 7.163569948840402\n",
      "Shannon: 7.642354847570101\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### * Generate array of pairs (node degree, neighbor degree)\n",
    "\n",
    "array = [[],[],[],[],[]]\n",
    "for i in range(5):\n",
    "    results = results_dict[f\"xz_{i}\"]\n",
    "    for r in results:\n",
    "        G = r.graph\n",
    "        for node in G.nodes:\n",
    "            degree = G.degree(node)\n",
    "            for nbr in G.neighbors(node):\n",
    "                nbr_degree = G.degree(nbr)\n",
    "                array[i].append([nbr_degree,degree])\n",
    "\n",
    "process_array(array,'Neighbor Degree','Node Degree')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon: 5.825843235346405\n",
      "Shannon: 7.338198723155596\n",
      "Shannon: 8.147985304437544\n",
      "Shannon: 8.668953282746674\n",
      "Shannon: 9.01945823070358\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### * Generate array (node tolerance, node neighborhood size)\n",
    "\n",
    "array = [[],[],[],[],[]]\n",
    "for i in range(5):\n",
    "    results = results_dict[f\"xz_{i}\"]\n",
    "    for r in results:\n",
    "        G = r.graph\n",
    "        for node in G.nodes:\n",
    "            if G.nodes[node]['object'] != None:\n",
    "                if G.nodes[node]['object'].tolerance_threshold != None:\n",
    "                    tol = G.nodes[node]['object'].tolerance_threshold\n",
    "                else:\n",
    "                    tol = -1\n",
    "            else:\n",
    "                tol = -2\n",
    "            degree = G.degree(node)\n",
    "            tol = discretize(tol)\n",
    "            array[i].append([degree,tol])\n",
    "\n",
    "process_array(array,'Degree','Tolerance')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon: 5.841695765766438\n",
      "Shannon: 6.1551075731955285\n",
      "Shannon: 6.138038154260999\n",
      "Shannon: 6.163115557170059\n",
      "Shannon: 6.253959261175155\n"
     ]
    }
   ],
   "source": [
    "\n",
    "         \n",
    "array = [[],[],[],[],[]]\n",
    "for i in range(5):\n",
    "    results = results_dict[f\"xz_{i}\"]\n",
    "    for r in results:\n",
    "        G = r.graph\n",
    "        for node in G.nodes:\n",
    "            if G.nodes[node]['object'] != None:\n",
    "                if G.nodes[node]['object'].tolerance_threshold != None:\n",
    "                    tol = G.nodes[node]['object'].tolerance_threshold\n",
    "                else:\n",
    "                    tol = -1\n",
    "            else:\n",
    "                tol = -2\n",
    "            tol = discretize(tol,bins=25)\n",
    "            node_color = G.nodes[node]['color']\n",
    "            node_degree = G.degree(node)\n",
    "            for nbr in G.neighbors(node):\n",
    "                if G.nodes[nbr]['color'] != G.nodes[node]['color']:\n",
    "                    if G.nodes[nbr]['object'] != None:\n",
    "                        if G.nodes[nbr]['object'].tolerance_threshold != None:\n",
    "                            nbr_tol = G.nodes[nbr]['object'].tolerance_threshold\n",
    "                        else:\n",
    "                            nbr_tol = -1\n",
    "                    else:\n",
    "                        nbr_tol = -2\n",
    "                    nbr_tol = discretize(nbr_tol,bins=25)\n",
    "                    nbr_color = G.nodes[nbr]['color']\n",
    "                    nbr_degree = G.degree(nbr)\n",
    "                    array[i].append([tol,nbr_tol])\n",
    "\n",
    "    \n",
    "process_array(array,'Neighbor Tolerance','Node Tolerance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Shannon: 0.5573926332340278\n",
      "1\n",
      "Shannon: 2.300460273501235\n",
      "2\n",
      "Shannon: 3.218087634768526\n",
      "3\n",
      "Shannon: 3.8049825385805276\n",
      "4\n",
      "Shannon: 4.190235070516265\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### * Generate Shannon entropy of node degree distribution.\n",
    "\n",
    "array = [[],[],[],[],[]]\n",
    "for i in range(5):\n",
    "    results = results_dict[f\"xz_{i}\"]\n",
    "    for r in results:\n",
    "        G = r.graph\n",
    "        for node in G.nodes:\n",
    "            degree = G.degree(node)\n",
    "            array[i].append(degree)\n",
    "prob_array = [[],[],[],[],[]]\n",
    "for i in range(5):\n",
    "    for val in set(array[i]):\n",
    "        prob_array[i].append(array[i].count(val)/len(array[i]))\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    measure_entropy(prob_array[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon: 0.5461050495535906\n",
      "Shannon: 4.5354520746550255\n",
      "Shannon: 6.444572054983051\n",
      "Shannon: 7.240590109365727\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[276], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m                             nbr_degree \u001b[39m=\u001b[39m G\u001b[39m.\u001b[39mdegree(nbr)\n\u001b[0;32m     25\u001b[0m                             array[i]\u001b[39m.\u001b[39mappend([node_degree,nbr_degree])\n\u001b[1;32m---> 26\u001b[0m process_array(array,\u001b[39m'\u001b[39;49m\u001b[39mNode Degree\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mNeighbor Degree\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[271], line 24\u001b[0m, in \u001b[0;36mprocess_array\u001b[1;34m(array, xlabel, ylabel)\u001b[0m\n\u001b[0;32m     22\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mPair\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m df[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m df[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     23\u001b[0m probabilities \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 24\u001b[0m sorted_symbols \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame([\u001b[39mstr\u001b[39m(\u001b[39msorted\u001b[39m((x[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m],x[\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]))) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows()])\n\u001b[0;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m symbol \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m(sorted_symbols[\u001b[39m0\u001b[39m]):\n\u001b[0;32m     26\u001b[0m     relative_freq \u001b[39m=\u001b[39m sorted_symbols[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mvalue_counts()[symbol]\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(sorted_symbols[\u001b[39m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[271], line 24\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     22\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mPair\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m df[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m df[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     23\u001b[0m probabilities \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 24\u001b[0m sorted_symbols \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame([\u001b[39mstr\u001b[39m(\u001b[39msorted\u001b[39m((x[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m],x[\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]))) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows()])\n\u001b[0;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m symbol \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m(sorted_symbols[\u001b[39m0\u001b[39m]):\n\u001b[0;32m     26\u001b[0m     relative_freq \u001b[39m=\u001b[39m sorted_symbols[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mvalue_counts()[symbol]\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(sorted_symbols[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py:1410\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1408\u001b[0m klass \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor_sliced\n\u001b[0;32m   1409\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues):\n\u001b[1;32m-> 1410\u001b[0m     s \u001b[39m=\u001b[39m klass(v, index\u001b[39m=\u001b[39;49mcolumns, name\u001b[39m=\u001b[39;49mk)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m)\n\u001b[0;32m   1411\u001b[0m     \u001b[39myield\u001b[39;00m k, s\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py:474\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    472\u001b[0m manager \u001b[39m=\u001b[39m get_option(\u001b[39m\"\u001b[39m\u001b[39mmode.data_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    473\u001b[0m \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mblock\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 474\u001b[0m     data \u001b[39m=\u001b[39m SingleBlockManager\u001b[39m.\u001b[39;49mfrom_array(data, index)\n\u001b[0;32m    475\u001b[0m \u001b[39melif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    476\u001b[0m     data \u001b[39m=\u001b[39m SingleArrayManager\u001b[39m.\u001b[39mfrom_array(data, index)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\internals\\managers.py:1936\u001b[0m, in \u001b[0;36mSingleBlockManager.from_array\u001b[1;34m(cls, array, index)\u001b[0m\n\u001b[0;32m   1931\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m   1932\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_array\u001b[39m(\u001b[39mcls\u001b[39m, array: ArrayLike, index: Index) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SingleBlockManager:\n\u001b[0;32m   1933\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1934\u001b[0m \u001b[39m    Constructor for if we have an array that is not yet a Block.\u001b[39;00m\n\u001b[0;32m   1935\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1936\u001b[0m     block \u001b[39m=\u001b[39m new_block(array, placement\u001b[39m=\u001b[39;49m\u001b[39mslice\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39mlen\u001b[39;49m(index)), ndim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m   1937\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(block, index)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\internals\\blocks.py:2178\u001b[0m, in \u001b[0;36mnew_block\u001b[1;34m(values, placement, ndim)\u001b[0m\n\u001b[0;32m   2175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(placement, BlockPlacement):\n\u001b[0;32m   2176\u001b[0m     placement \u001b[39m=\u001b[39m BlockPlacement(placement)\n\u001b[1;32m-> 2178\u001b[0m check_ndim(values, placement, ndim)\n\u001b[0;32m   2180\u001b[0m klass \u001b[39m=\u001b[39m get_block_type(values\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m   2182\u001b[0m values \u001b[39m=\u001b[39m maybe_coerce_values(values)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "array = [[],[],[],[],[]]\n",
    "for i in range(5):\n",
    "    results = results_dict[f\"xz_{i}\"]\n",
    "    for r in results:\n",
    "        G = r.graph\n",
    "        for node in G.nodes:\n",
    "            if G.nodes[node]['object'] != None:\n",
    "                if G.nodes[node]['object'].tolerance_threshold != None:\n",
    "                    tol = G.nodes[node]['object'].tolerance_threshold\n",
    "                else:\n",
    "                    tol = -1\n",
    "                tol = discretize(tol)\n",
    "                node_color = G.nodes[node]['color']\n",
    "                node_degree = G.degree(node)\n",
    "                for nbr in G.neighbors(node):\n",
    "                    if G.nodes[nbr]['color'] != G.nodes[node]['color']:\n",
    "                        if G.nodes[nbr]['object'] != None:\n",
    "                            if G.nodes[nbr]['object'].tolerance_threshold != None:\n",
    "                                nbr_tol = G.nodes[nbr]['object'].tolerance_threshold\n",
    "                            else:\n",
    "                                nbr_tol = -1\n",
    "                            nbr_tol = discretize(nbr_tol)\n",
    "                            nbr_color = G.nodes[nbr]['color']\n",
    "                            nbr_degree = G.degree(nbr)\n",
    "                            array[i].append([node_degree,nbr_degree])\n",
    "process_array(array,'Node Degree','Neighbor Degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "df = pd.DataFrame()\n",
    "tolerances = np.arange(.05,.76,.2)\n",
    "tolerances = [round(t,2) for t in tolerances]\n",
    "neighborhoods = np.arange(4,17,12)\n",
    "results = []\n",
    "for n in neighborhoods:\n",
    "    for epsilon in tolerances:\n",
    "        x = np.linspace(0,.5,100)\n",
    "        for p in x:\n",
    "            total = 0\n",
    "            for k in range(int(np.ceil(n * epsilon)),n+1):\n",
    "                total += math.comb(n,k)*(p**k)*(1-p)**(n-k)\n",
    "            results.append([p,total,n,epsilon])\n",
    "df = pd.DataFrame(results)\n",
    "df[3] = df[3].astype(float)\n",
    "df = df.rename(columns={0:\"p\",1:fr'P($\\epsilon$)',2:'Neighborhood Size',3:fr'$\\epsilon$'})\n",
    "sns.lineplot(df, x=\"p\",y=fr'P($\\epsilon$)',palette='colorblind',hue=fr'$\\epsilon$', style='Neighborhood Size', legend='full',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "df = pd.DataFrame()\n",
    "tolerances = [.05]\n",
    "tolerances = [round(t,2) for t in tolerances]\n",
    "neighborhoods = [4,16]\n",
    "results = []\n",
    "for epsilon in tolerances:\n",
    "    for n in neighborhoods:\n",
    "        x = np.linspace(0,.5,100)\n",
    "        for p in x:\n",
    "            total = 0\n",
    "            for k in range(int(np.ceil(n * epsilon)),n+1):\n",
    "                total += math.comb(n,k)*(p**k)*(1-p)**(n-k)\n",
    "            results.append([p,total,n,epsilon])\n",
    "df = pd.DataFrame(results)\n",
    "df[3] = df[3].astype(float)\n",
    "df = df.rename(columns={0:\"p\",1:fr'P($\\epsilon$)',2:'Neighborhood Size',3:fr'$\\epsilon$'})\n",
    "sns.lineplot(df, x=\"p\",y=fr'P($\\epsilon$)',palette='colorblind',hue='Neighborhood Size',style='Neighborhood Size', legend='full')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[\"sch_0\"][2].graph = results_dict[\"sch_0\"][28].graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
